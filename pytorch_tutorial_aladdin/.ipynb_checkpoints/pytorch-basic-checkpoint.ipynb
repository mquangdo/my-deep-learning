{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F  # Parameterless functions, like (some) activation functions\nimport torchvision.datasets as datasets  # Standard datasets\nimport torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\nfrom torch import optim  # For optimizers like SGD, Adam, etc.\nfrom torch import nn  # All neural network modules\nfrom torch.utils.data import (\n    DataLoader,\n)  # Gives easier dataset managment by creating mini batches etc.\nfrom tqdm import tqdm  # For nice progress bar!\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:42:16.999216Z","iopub.execute_input":"2024-12-04T16:42:17.000270Z","iopub.status.idle":"2024-12-04T16:42:21.924932Z","shell.execute_reply.started":"2024-12-04T16:42:17.000227Z","shell.execute_reply":"2024-12-04T16:42:21.923838Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 300)\n        self.fc2 = nn.Linear(300, 300)\n        self.fc3 =nn. Linear(300, 100)\n        self.fc4 = nn.Linear(100, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.sigmoid(x)\n        x = self.fc2(x)\n        x = F.sigmoid(x)\n        x = self.fc3(x)\n        x = F.sigmoid(x)\n        x = self.fc4(x)\n        x = F.softmax(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:49:52.630478Z","iopub.execute_input":"2024-12-04T16:49:52.631329Z","iopub.status.idle":"2024-12-04T16:49:52.637899Z","shell.execute_reply.started":"2024-12-04T16:49:52.631287Z","shell.execute_reply":"2024-12-04T16:49:52.636744Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"input_size = 784\nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:49:53.863325Z","iopub.execute_input":"2024-12-04T16:49:53.863767Z","iopub.status.idle":"2024-12-04T16:49:53.868950Z","shell.execute_reply.started":"2024-12-04T16:49:53.863731Z","shell.execute_reply":"2024-12-04T16:49:53.867968Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load Data\ntrain_dataset = datasets.MNIST(\n    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n)\ntest_dataset = datasets.MNIST(\n    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:49:55.313619Z","iopub.execute_input":"2024-12-04T16:49:55.313999Z","iopub.status.idle":"2024-12-04T16:49:55.408838Z","shell.execute_reply.started":"2024-12-04T16:49:55.313964Z","shell.execute_reply":"2024-12-04T16:49:55.407832Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = NeuralNetwork(input_size=input_size, num_classes=num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:50:38.184216Z","iopub.execute_input":"2024-12-04T16:50:38.185189Z","iopub.status.idle":"2024-12-04T16:50:38.193955Z","shell.execute_reply.started":"2024-12-04T16:50:38.185147Z","shell.execute_reply":"2024-12-04T16:50:38.192868Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Loss and optimizer\nloss = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:50:37.859840Z","iopub.execute_input":"2024-12-04T16:50:37.860218Z","iopub.status.idle":"2024-12-04T16:50:37.865183Z","shell.execute_reply.started":"2024-12-04T16:50:37.860184Z","shell.execute_reply":"2024-12-04T16:50:37.864133Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Train Network\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n        # Get to correct shape\n        data = data.reshape(data.shape[0], -1)\n\n        # Forward\n        scores = model(data)\n        loss = loss(scores, targets)\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient descent or adam step\n        optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:50:27.304681Z","iopub.execute_input":"2024-12-04T16:50:27.305047Z","iopub.status.idle":"2024-12-04T16:50:27.413824Z","shell.execute_reply.started":"2024-12-04T16:50:27.305013Z","shell.execute_reply":"2024-12-04T16:50:27.412468Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/938 [00:00<?, ?it/s]/tmp/ipykernel_24/2852392353.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  x = F.softmax(x)\n  0%|          | 1/938 [00:00<01:11, 13.03it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m      8\u001b[0m scores \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"],"ename":"TypeError","evalue":"'Tensor' object is not callable","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}